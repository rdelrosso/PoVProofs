#!/usr/bin/env python3
##
# Pymongo script which continuously writes records to MongoDB cluster primaries and then attempts
# to read the same records back from the primary. Used to compare the behaviour when simulating a
# complete cloud-region or data-centre failure where the replica-set's primary in one region/DC
# fails and another member of the replica set(s) in a different region/DC then takes over
# automatically as primary. Enables the difference between 'local-region' write safety and 'global'
# write safety to be demonstrated during failure scenarios. For the former (where the write
# concern is 0 & read concern is 1), during a failover, some writes persisted in one region and
# acknowledged, will never make it to the other region and will be lost, even though then were
# originally write-acknowledged. These can be detected by looking for lines beginning "ERROR" in
# the log file generated by this script, about 1 minute or so after a primary failure has been
# induced. When 'global' write safety has been enabled for this script (where both write concern
# and read concern will be set to majority), no such errors should occur due to failover, because
# written records that can be read, must have been propagated to a majority of regions and hence
# these writes will have tolerated a whole region failure.
#
# Usage (first ensure '.py' script is executable):
#   $ ./check-safety.py "<URL>" <LOCAL-AWARE | GLOBAL-AWARE>
#
# Where URL parameter is the MongoDB cluster address (specify it inside double-quotes), and the
# second parameter indicates whether to run the script with only local region/DC awareness (WC & RC
# = 1) or global awareness (WC & DC = Majority). Once the script is running, induce a failure on the
# primary of the replica-set(s).
#
# For example:
#   $ ./check_safety.py "mongodb+srv://mmyuser:mypassword@testcluster-abc.mongodb.net" LOCAL-AWARE
#
# The MongoDB database/collection created & used is 'test.records'
#
# Prerequisites:
# 1) Configure and run one MongoDB version 3.6+ replica-set or sharded cluster with different
#    replica-set members contained in different cloud-regions or DCs (easiest way to do this is use
#    Atlas)
# 2) Install latest PyMongo driver and DNS Python libraries, e.g:
#   $ pip3 install --user pymongo dnspython
#
# NOTE: The script spawns multiple processes in parallel to inject load. Some message may appear on
# the console, but the main diagnostics which indicate whether reads of writes have not been
# successful, are logged in a thread/process safe manner to the file 'consistency.log'. The script
# checks to see if records it couldn't initially read, are now accessible, 1 minute later, to be
# sure that failover has first been completed. Therefore, ensure you inspect the contents of this
# file at least 1 minute after primary failure has been induced.
##
import random
import sys
import time
import os
import contextlib
import json
import datetime
from multiprocessing import Process, Queue
from pymongo import MongoClient, WriteConcern, ASCENDING


####
# Main function to start up a number of OS processes that attempt write a record and then read
# same record back even if the target database cluster has failed over, logging failures in the
# file 'consistency.log'
####
def main():
    print()

    if len(sys.argv) < 3:
        print('Error:')
        print(f'2 arguments must be provided: 1) a <URL>, and 2) a {SAFETY_ARGS} flag\n')
        print('Example:')
        print('./check_safety.py "mongodb+srv://main_user:PASSWORD@testcluster-abcde.mongodb.net" '
              'GLOBAL-AWARE\n')
        sys.exit(1)

    mongodb_uri = sys.argv[1]
    ensure_safety_param = sys.argv[2].upper()
    global_aware = SAFETY.get(ensure_safety_param, None)

    if global_aware is None:
        print(f'Error:\nSecond argument must one of the following values: {SAFETY_ARGS}\n')
        sys.exit(1)

    print(f'Using URL: {mongodb_uri}')
    print(f'Has global-aware safety: {global_aware}')
    print(f'Launching {TOTAL_PROCESSES} processes')
    print(f'Run "tail -f" on the following log file during the run: {LOG_FILENAME} ...')

    with contextlib.suppress(FileNotFoundError):
        os.remove(LOG_FILENAME)

    # Create separate safe shared process queue for logging by other processes
    log_queue = Queue()
    log_process = Process(target=logger_process_run, args=(log_queue, STOP_TOKEN, LOG_FILENAME))
    log_process.start()
    log_queue.put(f'Starting consistency monitoring at {time.strftime("%H:%M:%S")}...\n\n')
    processesList = []

    # Create a set of OS processes to to load injection to leverage CPU cores
    for i in range(TOTAL_PROCESSES):
        process = Process(target=main_process_run, args=(i, mongodb_uri, log_queue, global_aware))
        processesList.append(process)

    try:
        # Start all processes
        for process in processesList:
            process.start()

        # Wait for all processes to finish
        for process in processesList:
            process.join()

        # Send queue message to end logging process
        log_queue.put(STOP_TOKEN)
        log_process.join()
    except KeyboardInterrupt:
        print(f'\nInterrupted - look for lines beginning with "ERROR" in file "{LOG_FILENAME}" '
              f'for an indication that writes weren\'t safely replicated across regions\n')
        keyboard_shutdown()


####
# Dequeue messages and write to single log file
# (means that message being written from one process doesn't interleave with messages being written
# from a different process)
####
def logger_process_run(log_queue, stop_token, logfile):
    try:
        with open(logfile, 'w') as f:
            while True:
                line = log_queue.get()

                if line == stop_token:
                    f.close()
                    break

                f.write(line)
                f.flush()
    except KeyboardInterrupt:
        keyboard_shutdown()


####
# Main process logic to connect to database, attempt to write a record to the database and then
# attempt to read the same record back from the database
####
def main_process_run(process_id, uri, log, global_aware):
    print(f'Process {process_id} connecting to MongoDB...')
    write_concern = 'majority' if global_aware else 0
    read_concern = 'majority' if global_aware else 'local'
    connection = MongoClient(host=uri, w=write_concern, readPreference='primary',
                             readConcernLevel=read_concern,
                             socketTimeoutMS=DEFAULT_TIMEOUT_MILLIS,
                             connectTimeoutMS=DEFAULT_TIMEOUT_MILLIS,
                             serverSelectionTimeoutMS=DEFAULT_TIMEOUT_MILLIS,
                             retryWrites=False, retryReads=False)
    collection = connection.test.records
    collection.drop()
    collection.create_index([('process_id', ASCENDING), ('index', ASCENDING)])
    collection.create_index('date_created', name=TTL_INDEX_NAME, expireAfterSeconds=7200)
    print('Ensured there is a TTL index to prune records after 2 hours\n')

    with open(LARGE_SUBDOC_FILENAME) as json_file:
        large_data = json.load(json_file)

    try:
        for index in range(LOOPS_TOTAL):
            try:
                if (insert_record(collection, process_id, index, log, large_data)):
                    doc = find_record_with_delayed_retry(collection, process_id, index, log)

                    if doc is None:
                        log.put(f'ERROR: INSERTED DOCUMENT NEVER FOUND - '
                                f'{time.strftime("%H:%M:%S")} - process_id {process_id} - index '
                                f'{index}\n')
                    elif DEBUG:
                        log.put(f'OK: INSERTED DOCUMENT FOUND - {time.strftime("%H:%M:%S")} '
                                f'- process_id {process_id} - index {index}\n')
            except Exception as e:
                print(f'Temporary issue whilst looping, error: {e}')
    except KeyboardInterrupt:
        keyboard_shutdown()


####
# Attempt to insert a new record into collection stamped with this process' ID
####
def insert_record(collection, process_id, index, log, large_data):
    try:
        collection.insert_one({'process_id': process_id, 'index': index,
                               'date_created': datetime.datetime.utcnow(),
                               'data': large_data})
        return True
    except Exception as e:
        log.put(f'INFO: UNABLE TO INSERT LIKELY DUE TO FAILED PRIMARY - {time.strftime("%H:%M:%S")}'
                f' - process_id {process_id} - index {index}: {e}\n')
        print(f'Temporary write issue likely due to failed primary - check log file, error: {e}')
        return False


####
# Attempt to find a record and if there is problem (e.g. a database primary failover has just
# occurred, sleep for a bit and then try again to see if the record appears on the new primary)
####
def find_record_with_delayed_retry(collection, process_id, index, log):
    doc = find_record(collection, process_id, index, log)

    if doc is None:
        log.put(f'INFO: DUE TO A MISSED FIND WILL TRY AGAIN AFTER A DELAY JUST IN CASE NEED TO '
                f'ALLOW TIME FOR NEW PRIMARY TO COME UP - {time.strftime("%H:%M:%S")} - process_id '
                f'{process_id} - index {index}\n')
        time.sleep(DEFAULT_READ_RETRY_WAIT_SECS)
        doc = find_record(collection, process_id, index, log)

        if doc is not None:
            log.put(f'INFO: FOUND DOC ON SECOND TRY, AFTER A SLEEP - {time.strftime("%H:%M:%S")} '
                    f'- process_id {process_id} - index {index}\n')

    return doc


####
# Attempt to find a record that was previously inserted by this process
####
def find_record(collection, process_id, index, log):
    try:
        return collection.find_one({'process_id': process_id, 'index': index}, {'_id': 1})
    except Exception as e:
        log.put(f'INFO: EXCEPTION TRYING TO DO FIND - {time.strftime("%H:%M:%S")} - process_id '
                f'{process_id} - index {index}: {e}\n')
        print(f'Temporary read issue likely due to failed primary - check log file, error: {e}')
        return None


####
# Swallow the verbiage that is spat out when using 'Ctrl-C' to kill the script
# and instead just print a simple line message
####
def keyboard_shutdown():
    try:
        sys.exit(0)
    except SystemExit as e:
        os._exit(0)


# Constants
STOP_TOKEN = 'EOF!!'
DEBUG = False
LOG_FILENAME = 'consistency.log'
LARGE_SUBDOC_FILENAME = 'large_subdoc.json'
TOTAL_PROCESSES = 8
DEFAULT_TIMEOUT_MILLIS = 8000
DEFAULT_READ_RETRY_WAIT_SECS = 60
LOOPS_TOTAL = 10000000
SAFETY = {
    'LOCAL-AWARE':  False,
    'GLOBAL-AWARE': True
}
SAFETY_ARGS = ' or '.join([key for key in SAFETY.keys()])
TTL_INDEX_NAME = 'date_created_ttl_index'


####
# Main
####
if __name__ == '__main__':
    main()
